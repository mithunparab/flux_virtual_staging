{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cea642e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4acf848a45ba4a0ab817ef69cbd92e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flux1-kontext-dev-Q3_K_M.gguf:   0%|          | 0.00/5.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see https://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to load weights from checkpoint file for '/workspace/.hf_home/hub/models--QuantStack--FLUX.1-Kontext-dev-GGUF/snapshots/2d083027732f81e5548620b57ac47da22d30aa5a/flux1-kontext-dev-Q3_K_M.gguf' at '/workspace/.hf_home/hub/models--QuantStack--FLUX.1-Kontext-dev-GGUF/snapshots/2d083027732f81e5548620b57ac47da22d30aa5a/flux1-kontext-dev-Q3_K_M.gguf'. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/diffusers/models/model_loading_utils.py:180\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, dduf_entries, disable_mmap, map_location)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m file_extension == GGUF_FILE_EXTENSION:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_gguf_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/diffusers/models/model_loading_utils.py:503\u001b[39m, in \u001b[36mload_gguf_checkpoint\u001b[39m\u001b[34m(gguf_checkpoint_path, return_tensors)\u001b[39m\n\u001b[32m    499\u001b[39m     logger.error(\n\u001b[32m    500\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLoading a GGUF checkpoint in PyTorch, requires both PyTorch and GGUF>=0.10.0 to be installed. Please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    501\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/ and https://github.com/ggerganov/llama.cpp/tree/master/gguf-py for installation instructions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease install torch and gguf>=0.10.0 to load a GGUF checkpoint in PyTorch.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    505\u001b[39m reader = GGUFReader(gguf_checkpoint_path)\n",
      "\u001b[31mImportError\u001b[39m: Please install torch and gguf>=0.10.0 to load a GGUF checkpoint in PyTorch.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/diffusers/models/model_loading_utils.py:197\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, dduf_entries, disable_mmap, map_location)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(checkpoint_file) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.startswith(\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    198\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    199\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou seem to have cloned a repository without having git-lfs installed. Please install \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    200\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgit-lfs and run `git lfs install` followed by `git lfs pull` in the folder \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    201\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33myou cloned.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    202\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:322\u001b[39m, in \u001b[36mdecode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x80 in position 201: invalid start byte",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m MAX_SEED = np.iinfo(np.int32).max\n\u001b[32m      9\u001b[39m ckpt_path = \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/QuantStack/FLUX.1-Kontext-dev-GGUF/blob/main/flux1-kontext-dev-Q3_K_M.gguf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m transformer = \u001b[43mFluxTransformer2DModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_single_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGGUFQuantizationConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mblack-forest-labs/FLUX.1-Kontext-dev\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m pipe = FluxKontextPipeline.from_pretrained(\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mblack-forest-labs/FLUX.1-Kontext-dev\u001b[39m\u001b[33m\"\u001b[39m, transformer=transformer, torch_dtype=torch.bfloat16\n\u001b[32m     21\u001b[39m ).to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/diffusers/loaders/single_file_model.py:297\u001b[39m, in \u001b[36mFromOriginalModelMixin.from_single_file\u001b[39m\u001b[34m(cls, pretrained_model_link_or_path_or_dict, **kwargs)\u001b[39m\n\u001b[32m    295\u001b[39m     checkpoint = pretrained_model_link_or_path_or_dict\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     checkpoint = \u001b[43mload_single_file_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_link_or_path_or_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable_mmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    309\u001b[39m     hf_quantizer = DiffusersAutoQuantizer.from_config(quantization_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/diffusers/loaders/single_file_utils.py:455\u001b[39m, in \u001b[36mload_single_file_checkpoint\u001b[39m\u001b[34m(pretrained_model_link_or_path, force_download, proxies, token, cache_dir, local_files_only, revision, disable_mmap, user_agent)\u001b[39m\n\u001b[32m    442\u001b[39m     repo_id, weights_name = _extract_repo_id_and_weights_name(pretrained_model_link_or_path)\n\u001b[32m    443\u001b[39m     pretrained_model_link_or_path = _get_model_file(\n\u001b[32m    444\u001b[39m         repo_id,\n\u001b[32m    445\u001b[39m         weights_name=weights_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    452\u001b[39m         user_agent=user_agent,\n\u001b[32m    453\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m checkpoint = \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_link_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_mmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable_mmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# some checkpoints contain the model state dict under a \"state_dict\" key\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstate_dict\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m checkpoint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/diffusers/models/model_loading_utils.py:209\u001b[39m, in \u001b[36mload_state_dict\u001b[39m\u001b[34m(checkpoint_file, dduf_entries, disable_mmap, map_location)\u001b[39m\n\u001b[32m    204\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    205\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to locate the file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m which is necessary to load this pretrained \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel. Make sure you have saved the model properly.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    210\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to load weights from checkpoint file for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m at \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    211\u001b[39m     )\n",
      "\u001b[31mOSError\u001b[39m: Unable to load weights from checkpoint file for '/workspace/.hf_home/hub/models--QuantStack--FLUX.1-Kontext-dev-GGUF/snapshots/2d083027732f81e5548620b57ac47da22d30aa5a/flux1-kontext-dev-Q3_K_M.gguf' at '/workspace/.hf_home/hub/models--QuantStack--FLUX.1-Kontext-dev-GGUF/snapshots/2d083027732f81e5548620b57ac47da22d30aa5a/flux1-kontext-dev-Q3_K_M.gguf'. "
     ]
    }
   ],
   "source": [
    "from diffusers import FluxKontextPipeline, FluxTransformer2DModel, GGUFQuantizationConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(42)\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "ckpt_path = \"https://huggingface.co/QuantStack/FLUX.1-Kontext-dev-GGUF/blob/main/flux1-kontext-dev-Q3_K_M.gguf\"\n",
    "\n",
    "transformer = FluxTransformer2DModel.from_single_file(\n",
    "    ckpt_path,\n",
    "    quantization_config=GGUFQuantizationConfig(compute_dtype=torch.bfloat16),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    config=\"black-forest-labs/FLUX.1-Kontext-dev\",\n",
    "    subfolder=\"transformer\",\n",
    ")\n",
    "\n",
    "pipe = FluxKontextPipeline.from_pretrained(\n",
    "    \"black-forest-labs/FLUX.1-Kontext-dev\", transformer=transformer, torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a4069",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
